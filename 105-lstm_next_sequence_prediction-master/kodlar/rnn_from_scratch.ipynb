{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 50 # 隐藏层神经元个数\n",
    "vocab_size  = 4 # 词汇表大小\n",
    "def init_orthogonal(param):\n",
    "    \"\"\"\n",
    "    正交初始化.\n",
    "    \"\"\"\n",
    "    if param.ndim < 2:\n",
    "        raise ValueError(\"参数维度必须大于2.\")\n",
    "    rows, cols = param.shape\n",
    "    new_param = np.random.randn(rows, cols) \n",
    "    if rows < cols:\n",
    "        new_param = new_param.T \n",
    "    # QR 矩阵分解， q为正交矩阵，r为上三角矩阵\n",
    "    q, r = np.linalg.qr(new_param)\n",
    "    # 让q均匀分布，https://arxiv.org/pdf/math-ph/0609050.pdf\n",
    "    d = np.diag(r, 0)\n",
    "    ph = np.sign(d)\n",
    "    q *= ph\n",
    "    if rows < cols:\n",
    "        q = q.T\n",
    "    new_param = q\n",
    "    return new_param\n",
    "\n",
    "def init_rnn(hidden_size, vocab_size):\n",
    "    \"\"\"\n",
    "    初始化循环神经网络参数.\n",
    "    Args:\n",
    "    hidden_size: 隐藏层神经元个数\n",
    "    vocab_size: 词汇表大小\n",
    "    \"\"\"\n",
    "    # 输入到隐藏层权重矩阵\n",
    "    U = np.zeros((hidden_size, vocab_size))\n",
    "    # 隐藏层到隐藏层权重矩阵\n",
    "    V = np.zeros((hidden_size, hidden_size))\n",
    "    # 隐藏层到输出层权重矩阵\n",
    "    W = np.zeros((vocab_size, hidden_size))\n",
    "    # 隐层bias\n",
    "    b_hidden = np.zeros((hidden_size, 1))\n",
    "    # 输出层bias\n",
    "    b_out = np.zeros((vocab_size, 1))\n",
    "    # 权重初始化\n",
    "    U = init_orthogonal(U)\n",
    "    V = init_orthogonal(V)\n",
    "    W = init_orthogonal(W)   \n",
    "    return U, V, W, b_hidden, b_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U: (50, 4)\n",
      "V: (50, 50)\n",
      "W: (4, 50)\n",
      "b_hidden: (50, 1)\n",
      "b_out: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "params = init_rnn(hidden_size=hidden_size, vocab_size=vocab_size)\n",
    "print('U:', params[0].shape)\n",
    "print('V:', params[1].shape)\n",
    "print('W:', params[2].shape)\n",
    "print('b_hidden:', params[3].shape)\n",
    "print('b_out:', params[4].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    x: 输入数据x\n",
    "    derivative: 如果为True则计算梯度\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = 1 / (1 + np.exp(-x_safe))\n",
    "    if derivative: \n",
    "        return f * (1 - f)\n",
    "    else: \n",
    "        return f\n",
    "def tanh(x, derivative=False):\n",
    "    x_safe = x + 1e-12\n",
    "    f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
    "    if derivative: \n",
    "        return 1-f**2\n",
    "    else: \n",
    "        return f\n",
    "def softmax(x, derivative=False):\n",
    "    x_safe = x + 1e-12\n",
    "    f = np.exp(x_safe) / np.sum(np.exp(x_safe))\n",
    "    \n",
    "    if derivative:\n",
    "        pass # 本文不计算softmax函数导数\n",
    "    else: \n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs, hidden_state, params):\n",
    "    \"\"\"\n",
    "    RNN前向传播计算\n",
    "    Args:\n",
    "    inputs: 输入序列\n",
    "    hidden_state: 初始化后的隐藏状态参数\n",
    "    params: RNN参数\n",
    "    \"\"\"\n",
    "    U, V, W, b_hidden, b_out = params\n",
    "    outputs, hidden_states = [], []\n",
    "    \n",
    "    for t in range(len(inputs)):\n",
    "        # 计算隐藏状态\n",
    "        hidden_state = tanh(np.dot(U, inputs[t]) + np.dot(V, hidden_state) + b_hidden)\n",
    "        # 计算输出\n",
    "        out = softmax(np.dot(W, hidden_state) + b_out)\n",
    "        # 保存中间结果\n",
    "        outputs.append(out)\n",
    "        hidden_states.append(hidden_state.copy())\n",
    "    return outputs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient_norm(grads, max_norm=0.25):\n",
    "    \"\"\"\n",
    "    梯度剪裁防止梯度爆炸\n",
    "    \"\"\" \n",
    "    max_norm = float(max_norm)\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        grad_norm = np.sum(np.power(grad, 2))\n",
    "        total_norm += grad_norm\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    if clip_coef < 1:\n",
    "        for grad in grads:\n",
    "            grad *= clip_coef\n",
    "    return grads\n",
    "def backward_pass(inputs, outputs, hidden_states, targets, params):\n",
    "    \"\"\"\n",
    "    后向传播\n",
    "    Args:\n",
    "     inputs: 序列输入\n",
    "     outputs: 输出\n",
    "     hidden_states: 隐藏状态\n",
    "     targets: 预测目标\n",
    "     params: RNN参数\n",
    "    \"\"\"\n",
    "    U, V, W, b_hidden, b_out = params\n",
    "    d_U, d_V, d_W = np.zeros_like(U), np.zeros_like(V), np.zeros_like(W)\n",
    "    d_b_hidden, d_b_out = np.zeros_like(b_hidden), np.zeros_like(b_out)\n",
    "    # 跟踪隐藏层偏导及损失\n",
    "    d_h_next = np.zeros_like(hidden_states[0])\n",
    "    loss = 0\n",
    "    # 对于输出序列当中每一个元素，反向遍历 s.t. t = N, N-1, ... 1, 0\n",
    "    for t in reversed(range(len(outputs))):\n",
    "        # 交叉熵损失\n",
    "        loss += -np.mean(np.log(outputs[t]+1e-12) * targets[t])\n",
    "        # d_loss/d_o\n",
    "        d_o = outputs[t].copy()\n",
    "        d_o[np.argmax(targets[t])] -= 1  \n",
    "        # d_o/d_w\n",
    "        d_W += np.dot(d_o, hidden_states[t].T)\n",
    "        d_b_out += d_o   \n",
    "        # d_o/d_h\n",
    "        d_h = np.dot(W.T, d_o) + d_h_next\n",
    "        # Backpropagate through non-linearity\n",
    "        d_f = tanh(hidden_states[t], derivative=True) * d_h\n",
    "        d_b_hidden += d_f  \n",
    "        # d_f/d_u\n",
    "        d_U += np.dot(d_f, inputs[t].T)\n",
    "        # d_f/d_v\n",
    "        d_V += np.dot(d_f, hidden_states[t-1].T)\n",
    "        d_h_next = np.dot(V.T, d_f)\n",
    "    grads = d_U, d_V, d_W, d_b_hidden, d_b_out    \n",
    "    # 梯度裁剪\n",
    "    grads = clip_gradient_norm(grads)\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, lr=1e-3):\n",
    "    # lr， learning rate， 学习率\n",
    "    for param, grad in zip(params, grads):\n",
    "        param -= lr * grad\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历训练集当中的序列数据\n",
    "for inputs, targets in training_set:\n",
    "    # 对数据进行预处理\n",
    "    inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "    targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "    # 针对每个训练样本，定义一个隐状态参数\n",
    "    hidden_state = np.zeros_like(hidden_state)\n",
    "    # 前向传播\n",
    "    outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params)\n",
    "    # 后向传播\n",
    "    loss, grads = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, params)\n",
    "    # 梯度更新\n",
    "    params = update_parameters(params, grads, lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入单元 + 隐藏单元 拼接后的大小\n",
    "z_size = hidden_size + vocab_size \n",
    "def init_lstm(hidden_size, vocab_size, z_size):\n",
    "    \"\"\"\n",
    "    LSTM网络初始化\n",
    "    Args:\n",
    "     hidden_size: 隐藏单元大小\n",
    "     vocab_size: 词汇表大小\n",
    "     z_size: 隐藏单元 + 输入单元大小\n",
    "    \"\"\"\n",
    "    # 遗忘门参数矩阵及偏置\n",
    "    W_f = np.random.randn(hidden_size, z_size)\n",
    "    b_f = np.zeros((hidden_size, 1))\n",
    "    # 记忆门参数矩阵及偏置\n",
    "    W_i = np.random.randn(hidden_size, z_size)\n",
    "    b_i = np.zeros((hidden_size, 1))\n",
    "    # 细胞状态参数矩阵及偏置\n",
    "    W_g = np.random.randn(hidden_size, z_size)\n",
    "    b_g = np.zeros((hidden_size, 1))\n",
    "    # 输出门参数矩阵及偏置\n",
    "    W_o = np.random.randn(hidden_size, z_size)\n",
    "    b_o = np.zeros((hidden_size, 1))\n",
    "    # 连接隐藏单元及输出的参数矩阵及偏置\n",
    "    W_v = np.random.randn(vocab_size, hidden_size)\n",
    "    b_v = np.zeros((vocab_size, 1))\n",
    "    # 正交参数初始化\n",
    "    W_f = init_orthogonal(W_f)\n",
    "    W_i = init_orthogonal(W_i)\n",
    "    W_g = init_orthogonal(W_g)\n",
    "    W_o = init_orthogonal(W_o)\n",
    "    W_v = init_orthogonal(W_v)\n",
    "    return W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inputs, h_prev, C_prev, p):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    inputs: [..., x, ...],  x表示t时刻的数据,  x的维度为(n_x, m).\n",
    "    h_prev： t-1时刻的隐藏状态数据，维度为 (n_a, m)\n",
    "    C_prev： t-1时刻的细胞状态数据，维度为 (n_a, m)\n",
    "    p：列表，包含LSTM初始化当中所有参数:\n",
    "                        W_f： (n_a, n_a + n_x)\n",
    "                        b_f： (n_a, 1)\n",
    "                        W_i： (n_a, n_a + n_x)\n",
    "                        b_i： (n_a, 1)\n",
    "                        W_g： (n_a, n_a + n_x)\n",
    "                        b_g： (n_a, 1)\n",
    "                        W_o： (n_a, n_a + n_x)\n",
    "                        b_o： (n_a, 1)\n",
    "                        W_v： (n_v, n_a)\n",
    "                        b_v： (n_v, 1)\n",
    "    Returns:\n",
    "    z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s：每一次前向传播后的中间输出\n",
    "    outputs：t时刻的预估值， 维度为(n_v, m)\n",
    "    \"\"\"\n",
    "    assert h_prev.shape == (hidden_size, 1)\n",
    "    assert C_prev.shape == (hidden_size, 1)\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
    "    # 保存各个单元的计算输出值\n",
    "    x_s, z_s, f_s, i_s,  = [], [] ,[], []\n",
    "    g_s, C_s, o_s, h_s = [], [] ,[], []\n",
    "    v_s, output_s =  [], [] \n",
    "    h_s.append(h_prev)\n",
    "    C_s.append(C_prev)\n",
    "    for x in inputs:\n",
    "        # 拼接输入及隐藏状态单元数据\n",
    "        z = np.row_stack((h_prev, x))\n",
    "        z_s.append(z)\n",
    "        # 遗忘门计算\n",
    "        f = sigmoid(np.dot(W_f, z) + b_f)\n",
    "        f_s.append(f)\n",
    "        # 输入门计算\n",
    "        i = sigmoid(np.dot(W_i, z) + b_i)\n",
    "        i_s.append(i) \n",
    "        # 细胞状态单元计算\n",
    "        g = tanh(np.dot(W_g, z) + b_g)\n",
    "        g_s.append(g)\n",
    "        # 下一时刻细胞状态计算\n",
    "        C_prev = f * C_prev + i * g \n",
    "        C_s.append(C_prev)\n",
    "        # 输出门计算\n",
    "        o = sigmoid(np.dot(W_o, z) + b_o)\n",
    "        o_s.append(o)\n",
    "        # 隐藏状态单元计算\n",
    "        h_prev = o * tanh(C_prev)\n",
    "        h_s.append(h_prev)\n",
    "        # 计算预估值\n",
    "        v = np.dot(W_v, h_prev) + b_v\n",
    "        v_s.append(v)\n",
    "        # softmax转换\n",
    "        output = softmax(v)\n",
    "        output_s.append(output)\n",
    "    return z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, output_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(z, f, i, g, C, o, h, v, outputs, targets, p = params):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    z，f，i，g，C，o，h，v，outputs：对应前向传播输出\n",
    "    targets： 目标值\n",
    "    p：W_f，b_f，W_i，b_i，W_g，b_g，W_o，b_o，W_v，b_v， LSTM参数\n",
    "    Returns:\n",
    "    loss：交叉熵损失\n",
    "    grads：p中参数的梯度\n",
    "    \"\"\"\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
    "    # 初始化梯度为0\n",
    "    W_f_d = np.zeros_like(W_f)\n",
    "    b_f_d = np.zeros_like(b_f)\n",
    "    \n",
    "    W_i_d = np.zeros_like(W_i)\n",
    "    b_i_d = np.zeros_like(b_i)\n",
    "\n",
    "    W_g_d = np.zeros_like(W_g)\n",
    "    b_g_d = np.zeros_like(b_g)\n",
    "\n",
    "    W_o_d = np.zeros_like(W_o)\n",
    "    b_o_d = np.zeros_like(b_o)\n",
    "\n",
    "    W_v_d = np.zeros_like(W_v)\n",
    "    b_v_d = np.zeros_like(b_v)\n",
    "    \n",
    "    dh_next = np.zeros_like(h[0])\n",
    "    dC_next = np.zeros_like(C[0])    \n",
    "    # 记录loss\n",
    "    loss = 0\n",
    "    for t in reversed(range(len(outputs))):\n",
    "        loss += -np.mean(np.log(outputs[t]) * targets[t])\n",
    "        C_prev= C[t-1]\n",
    "        #输出梯度\n",
    "        dv = np.copy(outputs[t])\n",
    "        dv[np.argmax(targets[t])] -= 1\n",
    "        #隐藏单元状态对输出的梯度\n",
    "        W_v_d += np.dot(dv, h[t].T)\n",
    "        b_v_d += dv\n",
    "        #隐藏单元梯度\n",
    "        dh = np.dot(W_v.T, dv)        \n",
    "        dh += dh_next\n",
    "        do = dh * tanh(C[t])\n",
    "        do = sigmoid(o[t], derivative=True)*do\n",
    "        #输入单元梯度\n",
    "        W_o_d += np.dot(do, z[t].T)\n",
    "        b_o_d += do\n",
    "        #下一时刻细胞状态梯度\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * o[t] * tanh(tanh(C[t]), derivative=True)\n",
    "        dg = dC * i[t]\n",
    "        dg = tanh(g[t], derivative=True) * dg \n",
    "        # 当前时刻细胞状态梯度\n",
    "        W_g_d += np.dot(dg, z[t].T)\n",
    "        b_g_d += dg\n",
    "        # 输入单元梯度\n",
    "        di = dC * g[t]\n",
    "        di = sigmoid(i[t], True) * di\n",
    "        W_i_d += np.dot(di, z[t].T)\n",
    "        b_i_d += di\n",
    "        # 遗忘单元梯度\n",
    "        df = dC * C_prev\n",
    "        df = sigmoid(f[t]) * df\n",
    "        W_f_d += np.dot(df, z[t].T)\n",
    "        b_f_d += df\n",
    "        # 上一时刻隐藏状态及细胞状态梯度\n",
    "        dz = (np.dot(W_f.T, df)\n",
    "             + np.dot(W_i.T, di)\n",
    "             + np.dot(W_g.T, dg)\n",
    "             + np.dot(W_o.T, do))\n",
    "        dh_prev = dz[:hidden_size, :]\n",
    "        dC_prev = f[t] * dC\n",
    "    grads= W_f_d, W_i_d, W_g_d, W_o_d, W_v_d, b_f_d, b_i_d, b_g_d, b_o_d, b_v_d\n",
    "    # 梯度裁剪\n",
    "    grads = clip_gradient_norm(grads)\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
