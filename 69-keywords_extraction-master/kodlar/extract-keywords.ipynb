{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv('data/papers.csv').head(500)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          500 non-null    int64 \n",
      " 1   year        500 non-null    int64 \n",
      " 2   title       500 non-null    object\n",
      " 3   event_type  0 non-null      object\n",
      " 4   pdf_name    500 non-null    object\n",
      " 5   abstract    500 non-null    object\n",
      " 6   paper_text  500 non-null    object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 27.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 abstracts are missing\n"
     ]
    }
   ],
   "source": [
    "print(\"{} abstracts are missing\".format(df[df['abstract']=='Abstract Missing']['abstract'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TITLE:Beating a Defender in Robotic Soccer: Memory-Based Learning of a '\n",
      " 'Continuous Function')\n",
      "'ABSTRACT:Abstract Missing'\n",
      "('FULL TEXT:Beating a Defender in Robotic Soccer:\\n'\n",
      " 'Memory-Based Learning of a Continuous\\n'\n",
      " 'FUnction\\n'\n",
      " 'Peter Stone\\n'\n",
      " 'Department of Computer Science\\n'\n",
      " 'Carnegie Mellon University\\n'\n",
      " 'Pittsburgh, PA 15213\\n'\n",
      " '\\n'\n",
      " 'Manuela Veloso\\n'\n",
      " 'Department of Computer Science\\n'\n",
      " 'Carnegie Mellon University\\n'\n",
      " 'Pittsburgh, PA 15213\\n'\n",
      " '\\n'\n",
      " 'Abstract\\n'\n",
      " \"Learning how to adjust to an opponent's position is critical to\\n\"\n",
      " 'the success of having intelligent agents collaborating towards the\\n'\n",
      " 'achievement of specific tasks in unfriendly environments. This paper '\n",
      " 'describes our work on a Memory-based technique for to choose\\n'\n",
      " 'an action based on a continuous-valued state attribute indicating\\n'\n",
      " 'the position of an opponent. We investigate the question of how an\\n'\n",
      " 'agent performs in nondeterministic variations of the training situations. '\n",
      " 'Our experiments indicate that when the random variations\\n'\n",
      " 'fall within some bound of the initial training, the agent performs\\n'\n",
      " 'better with some initial training rather than from a tabula-rasa.\\n'\n",
      " '\\n'\n",
      " '1\\n'\n",
      " '\\n'\n",
      " 'Introduction\\n'\n",
      " '\\n'\n",
      " 'One of the ultimate goals subjacent to the ')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "sample = 100\n",
    "pprint.pprint(\"TITLE:{}\".format(df['title'][sample]))\n",
    "pprint.pprint(\"ABSTRACT:{}\".format(df['abstract'][sample]))\n",
    "pprint.pprint(\"FULL TEXT:{}\".format(df['paper_text'][sample][:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains 7 columns: id, year, title, even_type, pdf_name, abstract and paper_text. We are mostly interested in the paper_text which include both title and abstract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "##Creating a list of custom stopwords\n",
    "new_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\", \n",
    "             \"show\", \"result\", \"large\", \n",
    "             \"also\", \"one\", \"two\", \"three\", \n",
    "             \"four\", \"five\", \"seven\",\"eight\",\"nine\"]\n",
    "stop_words = list(stop_words.union(new_words))\n",
    "\n",
    "def pre_process(text):\n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    #remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    ##Convert to list from string\n",
    "    text = text.split()\n",
    "    # remove stopwords\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    # remove words less than three letters\n",
    "    text = [word for word in text if len(word) >= 3]\n",
    "    # lemmatize\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    text = [lmtzr.lemmatize(word) for word in text]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.38 s, sys: 51.2 ms, total: 5.43 s\n",
      "Wall time: 5.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "docs = df['paper_text'].apply(lambda x:pre_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mean field theory layer visual cortex application artificial neural network christopher scofield center'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1][0:103]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.TF-IDF and Scikit-learn\n",
    "\n",
    "Based on the tutorial of [Kavita Ganesan](https://github.com/kavgan/nlp-in-practice/blob/master/tf-idf/Keyword%20Extraction%20with%20TF-IDF%20and%20SKlearn.ipynb)\n",
    "\n",
    "TF-IDF stands for Text Frequency Inverse Document Frequency. The importance of each word increases proportionally to the number of times a word appears in the document (Text Frequency - TF) but is offset by the frequency of the word in the corpus (Inverse Document Frequency - IDF). Using the tf-idf weighting scheme, the keywords are the words with the higherst TF-IDF score.\n",
    "\n",
    "### 1.1 CountVectorizer to create a vocabulary and generate word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.34 s, sys: 87.2 ms, total: 4.43 s\n",
      "Wall time: 4.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#docs = docs.tolist()\n",
    "#create a vocabulary of words, \n",
    "cv=CountVectorizer(max_df=0.95,         # ignore words that appear in 95% of documents\n",
    "                   max_features=10000,  # the size of the vocabulary\n",
    "                   ngram_range=(1,3)    # vocabulary contains single words, bigrams, trigrams\n",
    "                  )\n",
    "word_count_vector=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 TfidfTransformer to Compute Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.69 ms, sys: 170 µs, total: 1.86 ms\n",
      "Wall time: 1.32 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our IDF computed, we are now ready to compute TF-IDF and extract the top keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature names\n",
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "def get_keywords(idx, docs):\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs[idx]]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def print_results(idx,keywords, df):\n",
    "    # now print the results\n",
    "    print(\"\\n=====Title=====\")\n",
    "    print(df['title'][idx])\n",
    "    print(\"\\n=====Abstract=====\")\n",
    "    print(df['abstract'][idx])\n",
    "    print(\"\\n===Keywords===\")\n",
    "    for k in keywords:\n",
    "        print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=100\n",
    "keywords=get_keywords(idx, docs)\n",
    "# print_results(idx,keywords, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 0.091,\n",
       " 'agent': 0.393,\n",
       " 'defender': 0.584,\n",
       " 'learning': 0.105,\n",
       " 'mem': 0.206,\n",
       " 'memory': 0.357,\n",
       " 'nondeterministic': 0.099,\n",
       " 'speed': 0.133,\n",
       " 'stone': 0.121,\n",
       " 'veloso': 0.165}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not happy with result. I would like to add a filter that will remove similar keywords, or short keywords inside of complex ones. For instance, non-negative matrix factorization meets us 5 time: non negative matrix, negative matrix, nmf, matrix factorization, matrix. Adding a 4-grams does not change the situation. Similar keywords appears due to the fact that TF-IDF does not take into account the context, the keywords importance comes only from their frequencies relationship. Thus, TF-IDF is a quick, intuitive, but not the best way to extract keywords from the text. Let's look at other ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gensim implementation of TextRank summarization algorithm\n",
    "\n",
    "Gensim is a free Python library designed to automatically extract semantic topics from documents. The gensim implementation is based on the popular TextRank algorithm. \n",
    "\n",
    "[Documentation](https://radimrehurek.com/gensim/summarization/keywords.html)\n",
    "\n",
    "[Tutorial](https://rare-technologies.com/text-summarization-with-gensim/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Small text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (3.8.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (from gensim) (1.16.6)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (from gensim) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (1.9.66)\n",
      "Requirement already satisfied, skipping upgrade: boto in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.66 in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.12.189)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.2.0,>=0.1.10 in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.1.13)\n",
      "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.66->boto3->smart-open>=1.8.1->gensim) (0.16)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.66->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['factor',\n",
       " 'convergence',\n",
       " 'rescaling',\n",
       " 'multiplicative',\n",
       " 'function',\n",
       " 'kullback',\n",
       " 'gradient',\n",
       " 'algorithm',\n",
       " 'matrix',\n",
       " 'useful decomposition',\n",
       " 'data',\n",
       " 'multivariate',\n",
       " 'squares']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "text = \"Non-negative matrix factorization (NMF) has previously been shown to \" + \\\n",
    "\"be a useful decomposition for multivariate data. Two different multiplicative \" + \\\n",
    "\"algorithms for NMF are analyzed. They differ only slightly in the \" + \\\n",
    "\"multiplicative factor used in the update rules. One algorithm can be shown to \" + \\\n",
    "\"minimize the conventional least squares error while the other minimizes the  \" + \\\n",
    "\"generalized Kullback-Leibler divergence. The monotonic convergence of both  \" + \\\n",
    "\"algorithms can be proven using an auxiliary function analogous to that used \" + \\\n",
    "\"for proving convergence of the Expectation-Maximization algorithm. The algorithms  \" + \\\n",
    "\"can also be interpreted as diagonally rescaled gradient descent, where the  \" + \\\n",
    "\"rescaling factor is optimally chosen to ensure convergence.\"\n",
    "gensim.summarization.keywords(text, \n",
    "         ratio=0.5,               # use 50% of original text\n",
    "         words=None,              # Number of returned words\n",
    "         split=True,              # Whether split keywords\n",
    "         scores=False,            # Whether score of keyword\n",
    "         pos_filter=('NN', 'JJ'), # Part of speech (nouns, adjectives etc.) filters\n",
    "         lemmatize=True,         # If True - lemmatize words\n",
    "         deacc=True)              # If True - remove accentuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY:  ['Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition for multivariate data.', 'Two different multiplicative algorithms for NMF are analyzed.', 'They differ only slightly in the multiplicative factor used in the update rules.']\n"
     ]
    }
   ],
   "source": [
    "print(\"SUMMARY: \", gensim.summarization.summarize(text,\n",
    "                                                  ratio = 0.5,\n",
    "                                                  split = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Large text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_gensim(idx, docs):\n",
    "    \n",
    "    keywords=gensim.summarization.keywords(docs[idx], \n",
    "                                  ratio=None, \n",
    "                                  words=10,         \n",
    "                                  split=True,             \n",
    "                                  scores=False,           \n",
    "                                  pos_filter=None, \n",
    "                                  lemmatize=True,         \n",
    "                                  deacc=True)              \n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def print_results_gensim(idx,keywords, df):\n",
    "    # now print the results\n",
    "    print(\"\\n=====Title=====\")\n",
    "    print(df['title'][idx])\n",
    "    print(\"\\n=====Abstract=====\")\n",
    "    print(df['abstract'][idx])\n",
    "    print(\"\\n===Keywords===\")\n",
    "    for k in keywords:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "Beating a Defender in Robotic Soccer: Memory-Based Learning of a Continuous Function\n",
      "\n",
      "=====Abstract=====\n",
      "Abstract Missing\n",
      "\n",
      "===Keywords===\n",
      "learn\n",
      "memory\n",
      "agent\n",
      "defender\n",
      "positive\n",
      "resulting\n",
      "experience\n",
      "representing\n",
      "train\n",
      "action\n"
     ]
    }
   ],
   "source": [
    "idx=100\n",
    "keywords=get_keywords_gensim(idx, docs)\n",
    "print_results_gensim(idx,keywords, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keywords highlight the main point , but still miss valuable information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Python implementation of the Rapid Automatic Keyword Extraction algorithm (RAKE) using NLTK\n",
    "\n",
    "[Documentation](https://github.com/csurfer/rake-nltk)\n",
    "\n",
    "### Setup using pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rake-nltk in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (1.0.4)\r\n",
      "Requirement already satisfied: nltk in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (from rake-nltk) (3.4.5)\r\n",
      "Requirement already satisfied: six in /home/waves8/anaconda3/envs/thy_env/lib/python3.6/site-packages (from nltk->rake-nltk) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install rake-nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### or directly from the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/csurfer/rake-nltk.git\n",
    "# !python rake-nltk/setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Small text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Non-negative matrix factorization (NMF) has previously been shown to \" + \\\n",
    "\"be a useful decomposition for multivariate data. Two different multiplicative \" + \\\n",
    "\"algorithms for NMF are analyzed. They differ only slightly in the \" + \\\n",
    "\"multiplicative factor used in the update rules. One algorithm can be shown to \" + \\\n",
    "\"minimize the conventional least squares error while the other minimizes the  \" + \\\n",
    "\"generalized Kullback-Leibler divergence. The monotonic convergence of both  \" + \\\n",
    "\"algorithms can be proven using an auxiliary function analogous to that used \" + \\\n",
    "\"for proving convergence of the Expectation-Maximization algorithm. The algorithms  \" + \\\n",
    "\"can also be interpreted as diagonally rescaled gradient descent, where the  \" + \\\n",
    "\"rescaling factor is optimally chosen to ensure convergence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16.0, 'diagonally rescaled gradient descent'),\n",
       " (16.0, 'conventional least squares error'),\n",
       " (14.0, 'two different multiplicative algorithms'),\n",
       " (9.0, 'negative matrix factorization'),\n",
       " (9.0, 'auxiliary function analogous'),\n",
       " (8.0, 'multiplicative factor used'),\n",
       " (4.5, 'rescaling factor'),\n",
       " (4.0, 'useful decomposition'),\n",
       " (4.0, 'update rules'),\n",
       " (4.0, 'proving convergence')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "r = Rake()\n",
    "r.extract_keywords_from_text(text)\n",
    "r.get_ranked_phrases_with_scores()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! We see well interbretable machine learning terminology! But why diagonally rescaled gradient descent is more important than negative matrix factorization? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Large Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "Beating a Defender in Robotic Soccer: Memory-Based Learning of a Continuous Function\n",
      "\n",
      "=====Abstract=====\n",
      "Abstract Missing\n",
      "\n",
      "===Keywords===\n",
      "ial environment assume agent specific goal achieve conduct investigation framework team agent compete game robotic soccer real system model car remotely controlled board computer development research currently conducted simulator physical system simulator real world system based closely system designed laboratory computationalintelligence university british columbia sahota sahota simulator facilitates control number car ball within designated playing area care taken ensure simulator model real world response friction conserva memory based learning continuous function tion momentum etc closely possible show simulator graphic graphic view simulator initial position experiment paper teammate black remains stationary defender white move small circle different speed ball move either directly towards goal towards teammate position ball represents position learning agent focus question learning choose among action presence adversary paper describes work applying memory based supervised learni\n"
     ]
    }
   ],
   "source": [
    "def get_keywords_rake(idx, docs, n=10):\n",
    "    # Uses stopwords for english from NLTK, and all puntuation characters by default\n",
    "    r = Rake()\n",
    "    \n",
    "    # Extraction given the text.\n",
    "    r.extract_keywords_from_text(docs[idx][1000:2000])\n",
    "    \n",
    "    # To get keyword phrases ranked highest to lowest.\n",
    "    keywords = r.get_ranked_phrases()[0:n]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def print_results(idx,keywords, df):\n",
    "    # now print the results\n",
    "    print(\"\\n=====Title=====\")\n",
    "    print(df['title'][idx])\n",
    "    print(\"\\n=====Abstract=====\")\n",
    "    print(df['abstract'][idx])\n",
    "    print(\"\\n===Keywords===\")\n",
    "    for k in keywords:\n",
    "        print(k)\n",
    "\n",
    "idx=100\n",
    "keywords = get_keywords_rake(idx, docs, n=10)\n",
    "print_results(idx, keywords, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oups! Something goes wrong! Algorithm does not work for the preprocessed text without punctuations. Let's treat the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "Beating a Defender in Robotic Soccer: Memory-Based Learning of a Continuous Function\n",
      "\n",
      "=====Abstract=====\n",
      "Abstract Missing\n",
      "\n",
      "===Keywords===\n",
      "model cars remotely controlled\n",
      "machine learning perspective\n",
      "designated playing area\n",
      "multiple agents collaborating\n",
      "research works towards\n",
      "intelligent agents\n",
      "agents compete\n",
      "world system\n",
      "systems designed\n",
      "specific goal\n"
     ]
    }
   ],
   "source": [
    "idx=100\n",
    "keywords = get_keywords_rake(idx, df['paper_text'], n=10)\n",
    "print_results(idx, keywords, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presented implementation works well on sentences, but it is not flexible enough for large text. However, those who are interested in RANK can expand the capabilities of this code to their needs. We will consider next options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Yet Another Keyword Extractor (Yake)\n",
    "\n",
    "[Documentation](https://github.com/LIAAD/yake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "Beating a Defender in Robotic Soccer: Memory-Based Learning of a Continuous Function\n",
      "\n",
      "=====Abstract=====\n",
      "Abstract Missing\n",
      "\n",
      "===Keywords===\n",
      "('non-negative matrix factorization', 0.0041066275750552455)\n",
      "('non-negative matrix', 0.026529705128479162)\n",
      "('matrix factorization', 0.026529705128479162)\n",
      "('multivariate data', 0.026529705128479162)\n",
      "('decomposition for multivariate', 0.03127464030655176)\n",
      "('nmf', 0.06699743201311577)\n",
      "('nmf are analyzed', 0.11148839518508058)\n",
      "('algorithms', 0.13323194577601624)\n",
      "('previously been shown', 0.1372005684192386)\n",
      "('non-negative', 0.1484061535685674)\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "\n",
    "def get_keywords_yake(idx, docs):\n",
    "    y = yake.KeywordExtractor(lan='en',          # language\n",
    "                             n = 3,              # n-gram size\n",
    "                             dedupLim = 0.9,     # deduplicationthresold\n",
    "                             dedupFunc = 'seqm', #  deduplication algorithm\n",
    "                             windowsSize = 1,\n",
    "                             top = 10,           # number of keys\n",
    "                             features=None)           \n",
    "    \n",
    "    keywords = y.extract_keywords(text)\n",
    "    return keywords\n",
    "\n",
    "idx=100\n",
    "keywords = get_keywords_yake(idx, docs[idx])\n",
    "print_results(idx, keywords, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key phrases are repeated, and the text needs pre-processing to remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Keyphrases extraction using pke\n",
    "\n",
    "`pke` an open source python-based keyphrase extraction toolkit. It provides an end-to-end keyphrase extraction pipeline in which each component can be easily modified or extended to develop new models.\n",
    "\n",
    "`pke` currently implements the following keyphrase extraction models:\n",
    "\n",
    "* Unsupervised models\n",
    "  * Statistical models\n",
    "    * TfIdf [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#tfidf)]\n",
    "    * KPMiner [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#kpminer), [article by (El-Beltagy and Rafea, 2010)](http://www.aclweb.org/anthology/S10-1041.pdf)]\n",
    "    * YAKE [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#yake), [article by (Campos et al., 2020)](https://doi.org/10.1016/j.ins.2019.09.013)]\n",
    "  * Graph-based models\n",
    "    * TextRank [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#textrank), [article by (Mihalcea and Tarau, 2004)](http://www.aclweb.org/anthology/W04-3252.pdf)]\n",
    "    * SingleRank  [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#singlerank), [article by (Wan and Xiao, 2008)](http://www.aclweb.org/anthology/C08-1122.pdf)]\n",
    "    * TopicRank [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#topicrank), [article by (Bougouin et al., 2013)](http://aclweb.org/anthology/I13-1062.pdf)]\n",
    "    * TopicalPageRank [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#topicalpagerank), [article by (Sterckx et al., 2015)](http://users.intec.ugent.be/cdvelder/papers/2015/sterckx2015wwwb.pdf)]\n",
    "    * PositionRank [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#positionrank), [article by (Florescu and Caragea, 2017)](http://www.aclweb.org/anthology/P17-1102.pdf)]\n",
    "    * MultipartiteRank [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#multipartiterank), [article by (Boudin, 2018)](https://arxiv.org/abs/1803.08721)]\n",
    "* Supervised models\n",
    "  * Feature-based models\n",
    "    * Kea [[documentation](https://boudinfl.github.io/pke/build/html/supervised.html#kea), [article by (Witten et al., 2005)](https://www.cs.waikato.ac.nz/ml/publications/2005/chap_Witten-et-al_Windows.pdf)]\n",
    "    * WINGNUS [[documentation](https://boudinfl.github.io/pke/build/html/supervised.html#wingnus), [article by (Nguyen and Luong, 2010)](http://www.aclweb.org/anthology/S10-1035.pdf)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/boudinfl/pke.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1  SingleRank\n",
    "\n",
    "This model is an extension of the TextRank model that uses the number of co-occurrences to weigh edges in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "Beating a Defender in Robotic Soccer: Memory-Based Learning of a Continuous Function\n",
      "\n",
      "=====Abstract=====\n",
      "Abstract Missing\n",
      "\n",
      "===Keywords===\n",
      "different multiplicative algorithms\n",
      "non - negative matrix factorization\n",
      "conventional least squares error\n",
      "multiplicative factor\n",
      "monotonic convergence\n",
      "algorithms\n",
      "other minimizes\n",
      "maximization algorithm\n",
      "auxiliary function analogous\n",
      "rescaling factor\n"
     ]
    }
   ],
   "source": [
    "# define the set of valid Part-of-Speeches\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# 1. create a SingleRank extractor.\n",
    "extractor = pke.unsupervised.SingleRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text,\n",
    "                        language='en',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. select the longest sequences of nouns and adjectives as candidates.\n",
    "extractor.candidate_selection(pos=pos)\n",
    "\n",
    "# 4. weight the candidates using the sum of their word's scores that are\n",
    "#    computed using random walk. In the graph, nodes are words of\n",
    "#    certain part-of-speech (nouns and adjectives) that are connected if\n",
    "#    they occur in a window of 10 words.\n",
    "extractor.candidate_weighting(window=10,\n",
    "                              pos=pos)\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "\n",
    "idx = 100\n",
    "# now print the results\n",
    "print(\"\\n=====Title=====\")\n",
    "print(df['title'][idx])\n",
    "print(\"\\n=====Abstract=====\")\n",
    "print(df['abstract'][idx])\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keyphrases:\n",
    "    print(k[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 TopicRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "We’re living through a fascinating era of rapid change for the blockbuster movie model. America producers, eager to get their $200 million movies into the lucrative Chinese market, are increasingly looking for Chinese production partners, shooting in Chinese locations, and adding China-friendly characters and plotlines to American movies, even including extra scenes just for the Chinese cuts of films. But simultaneously, China and other countries are moving toward the blockbuster model themselves, creating homegrown films that don’t need to involve American partners at all.\n",
    "And just as American films attempt to find paydays in foreign markets, foreign blockbusters are coming to America. The Wandering Earth, described as China’s first big-budget science fiction thriller, quietly made it onto screens at AMC theaters in North America this weekend, and it shows a new side of Chinese filmmaking — one focused toward futuristic spectacles rather than China’s traditionally grand, massive historical epics. At the same time, The Wandering Earth feels like a throwback to a few familiar eras of American filmmaking. While the film’s cast, setting, and tone are all Chinese, longtime science fiction fans are going to see a lot on the screen that reminds them of other movies, for better or worse.\n",
    "The film, based on a short story by Three-Body Problem author Cixin Liu, lays out a crisis of unprecedented proportions: the sun has become unstable, and within a hundred years, it will expand to consume Earth. Within 300, the entire solar system will be gone. Earth’s governments rally and unite to face the problem, and come up with a novel solution: they speckle the planet with 10,000 gigantic jets, and blast it out of its orbit and off on a hundred-generation journey to a new home 4.2 light-years away. The idea is to use Jupiter’s gravitational well to pick up speed for the trip, but a malfunction of the Earth Engine system leaves the planet caught in Jupiter’s gravity, and gradually being pulled toward destruction. A frantic group of workers have to scramble to reactivate the jets and correct the Earth’s course.\n",
    "The action takes place in two arenas simultaneously. On the Earth’s frigid surface, self-proclaimed genius Liu Qi (Qu Chuxiao) and his younger adopted sister Han Duoduo (Zhao Jinmai) get roped into the rescue efforts after they run away from home. Han is just curious to see the planet’s surface — most of humanity now lives in crowded underground cities, and the surface is for workers only — but Liu Qi is nursing a deeper grudge against his astronaut father Liu Peiqiang (longtime martial-arts movie star Wu Jing) and grandfather (Ng Man-tat, whom Western audiences might recognize from Stephen Chow’s Shaolin Soccer). When Liu Qi was a child, his father moved to a newly-built international space station, designed to move ahead of Earth as a guide and pathfinder. Now an adult, Liu Qi feels his father abandoned him, and wants to strike out independently.\n",
    "Meanwhile, on the space station, Liu Peiqiang is ironically a day away from completing his 17-year tour of duty and returning to Earth and his family when the crisis hits. The station’s artificial intelligence, MOSS, insists on putting the station’s personnel in hibernation to save energy, but Liu Peiqiang realizes the computer has a secret agenda, and he and a Russian cosmonaut set out to defy it.\n",
    "The entire space plot may feel suspiciously familiar to American audiences, who have a strong emotional touchstone when it comes to a calm-voiced computer in space telling a desperate astronaut that it can’t obey his orders, even when human lives are on the line, because it has orders of its own. MOSS even looks something like the HAL 9000 from 2001: A Space Odyssey: it’s represented as a red light on a gimbled panel, like a single unblinking, judgmental red eye. But a good deal of Liu Peiqiang’s space adventure also plays out like a sequence from Alfonso Cuarón’s 2013 Oscar-winner Gravity, with dizzying sequences of astronauts trying to navigate clouds of debris and find handholds on a treacherous moving station while tumbling through space.\n",
    "Meanwhile, the Earthside half of the mission resembles nothing so much as the 2003 nonsense-thriller The Core, about a team trying to drill their way to the center of the Earth to set the planet’s core spinning again. Liu Qi and Han pick up a few distinctive allies along the way, including biracial Chinese-Australian gadabout Tim (viral video star Mike Sui), but mostly, the characters are drawn as blandly and broadly as in any American action movie, and a fair number of them get killed along the journey without ever having developed enough personality for audiences to feel the loss.\n",
    "Pretty much any flaw The Wandering Earth can claim — flashy action scenes without much substance, a marked bent toward sticky sentimentality, an insistently pushy score that demands emotional response from the audience at every given moment — are familiar flaws from past blockbusters. Where the film really stands out, though, is in its eye for grandiose spectacle. Director Frant Gwo gives the film a surprising stateliness, especially in the scenes of the mobile Earth wandering the cosmos, wreathed in tiny blue jets that leave eerie space-contrails behind. His attention to detail is marvelous — in scenes where characters stand on Earth’s surface, contemplating Jupiter’s malicious beauty, the swirling colors of the Great Red Spot are clearly visible in reflections in their suit helmets.\n",
    "No matter how familiar the plot beats feel, that level of attention not just to functional special effects, but to outright beauty, makes The Wandering Earth memorable. Not every CGI sequence is aesthetically impeccable — sequences like a vehicle chase through a frozen Shanghai sometimes look brittle and false. But everything having to do with Jupiter, Earth as seen from space, and the space station subplot is visually sumptuous. This is frequently a gorgeously rendered film, with an emphasis on intimidating space vistas that will look tremendous on IMAX screens.\n",
    "And while the constant attempts to flee the destructive power of changing weather have their own echoes in past films, from The Day After Tomorrow to 2012, Gwo mostly keeps the action tight and propulsive. The Wandering Earth is frequently breathless, though the action occasionally gets a little muddled in editing. At times, particularly on the surface scenes where everyone is wearing identical pressure suits, it can be easy to lose track of which character is where. It’s often easy to feel that Gwo cares more about the collective rescue project than about any individual character — potentially a value that will work better for Chinese audiences than American viewers, who are looking for a single standout hero to root for.\n",
    "But the film’s biggest strengths are in its quieter moments, where Gwo takes the time to contemplate Jupiter’s gravity well slowly deepening its pull on Earth’s atmosphere, or Liu Qi staring up, awestruck, at the gas giant dwarfing his home. In those chilly sequences, the film calls back to an older tradition of slower science fiction, in epic-scale classics like 1951’s When Worlds Collide or 1956’s Forbidden Planet. The interludes are brief, but they’re a welcome respite from chase sequences and destruction.\n",
    "The Wandering Earth gets pretty goofy at times, with jokes about Tim’s heritage, or Liu Qi’s inexperienced driving and overwhelming arrogance, or with high-speed banter over an impossibly long technical manual that no one has time to digest in the middle of an emergency. At times, the humor is even a little dry, as when MOSS responds to Liu Peiqiang’s repeated rebellions with a passive-aggressive “Will all violators stop contact immediately with Earth?” But Gwo finds time for majesty as well, and makes a point of considering the problem on a global scale, rather than just focusing on the few desperate strivers who’ve tied the Earth’s potential destruction into their own personal issues.\n",
    "Much like the Russian space blockbuster Salyut-7 was a fascinating look into the cultural differences between American films and their Russian equivalents, The Wandering Earth feels like a telling illustration of the similarities and differences between Chinese and American values. Gwo’s film is full of images and moments that will be familiar to American audiences, and it has an equally familiar preoccupation with the importance of family connections, and the nobility of sacrifice. But it also puts a strong focus on global collective action, on the need for international cooperation, and for the will of the group over the will of the individual.\n",
    "None of these things will be inherently alien to American viewers, who may experience The Wandering Earth as a best-of mash-up of past science fiction films, just with less-familiar faces in the lead roles. But as China gets into the action-blockbuster business, it’ll continue to be fascinating to see how the country brings its own distinctive voices and talents into a global market. The Wandering Earth feels like the same kind of projects American filmmakers are making — accessible, thrill-focused, and at least somewhat generic, in an attempt to go down easy with any audience. But there’s enough specific personality in it to point to a future of more nationally inflected blockbusters. Once every country is making would-be international crossovers, the strongest appeal may come from the most distinctive, personal visions with the most to say about the cultures they come from.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "Beating a Defender in Robotic Soccer: Memory-Based Learning of a Continuous Function\n",
      "\n",
      "=====Abstract=====\n",
      "Abstract Missing\n",
      "\n",
      "===Keywords===\n",
      "wandering earth\n",
      "films\n",
      "entire space plot\n",
      "genius liu qi\n",
      "chinese production partners\n",
      "times\n",
      "sequence\n",
      "director frant gwo\n",
      "action\n",
      "international space station\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 1. create a TopicRank extractor.\n",
    "extractor = pke.unsupervised.TopicRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text)\n",
    "\n",
    "# 3. select the longest sequences of nouns and adjectives, that do\n",
    "#    not contain punctuation marks or stopwords as candidates.\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "stoplist = list(string.punctuation)\n",
    "stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "stoplist += stopwords.words('english')\n",
    "extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
    "\n",
    "# 4. build topics by grouping candidates with HAC (average linkage,\n",
    "#    threshold of 1/4 of shared stems). Weight the topics using random\n",
    "#    walk, and select the first occuring candidate from each topic.\n",
    "extractor.candidate_weighting(threshold=0.74, method='average')\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "\n",
    "idx = 100\n",
    "# now print the results\n",
    "print(\"\\n=====Title=====\")\n",
    "print(df['title'][idx])\n",
    "print(\"\\n=====Abstract=====\")\n",
    "print(df['abstract'][idx])\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keyphrases:\n",
    "    print(k[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wandering earth', 0.04454298297904837),\n",
       " ('films', 0.026726986229525875),\n",
       " ('entire space plot', 0.01654283261399369),\n",
       " ('genius liu qi', 0.015725128443376625),\n",
       " ('chinese production partners', 0.015229488820999334),\n",
       " ('times', 0.014073050308177645),\n",
       " ('sequence', 0.013950319757885715),\n",
       " ('director frant gwo', 0.013702145983472108),\n",
       " ('action', 0.013037147499461419),\n",
       " ('international space station', 0.01283682037083518)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyphrases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thy_env",
   "language": "python",
   "name": "thy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
